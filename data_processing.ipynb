{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Olist Data Processing and RAG System\n",
        "\n",
        "This notebook processes the Olist merged CSV file, creates unique order IDs, builds a relational database, and generates vector embeddings for RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the CSV file\n",
        "print(\"Loading CSV file...\")\n",
        "df = pd.read_csv('olist_full_merged.csv', low_memory=False)\n",
        "print(f\"Loaded {len(df)} rows with {len(df.columns)} columns\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unique order_id based on order_id + customer_id + seller_id combination\n",
        "# If two rows have the same combination, they get the same unique ID\n",
        "print(\"Creating unique order IDs...\")\n",
        "\n",
        "# Create a combination key\n",
        "df['order_customer_seller_key'] = (\n",
        "    df['order_id'].astype(str) + '_' + \n",
        "    df['customer_id'].astype(str) + '_' + \n",
        "    df['seller_id'].astype(str)\n",
        ")\n",
        "\n",
        "# Create a mapping of unique combinations to sequential IDs\n",
        "unique_combinations = df['order_customer_seller_key'].unique()\n",
        "combination_to_id = {combo: idx + 1 for idx, combo in enumerate(unique_combinations)}\n",
        "\n",
        "# Map the new unique IDs\n",
        "df['unique_order_id'] = df['order_customer_seller_key'].map(combination_to_id)\n",
        "\n",
        "# Replace the original order_id with the unique one\n",
        "df['order_id'] = df['unique_order_id']\n",
        "df = df.drop(['order_customer_seller_key', 'unique_order_id'], axis=1)\n",
        "\n",
        "print(f\"Created {len(unique_combinations)} unique order IDs\")\n",
        "print(f\"Sample of new order_ids: {df['order_id'].head(10).tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the processed CSV\n",
        "print(\"Saving processed CSV...\")\n",
        "df.to_csv('olist_processed.csv', index=False)\n",
        "print(\"Saved to olist_processed.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create relational database\n",
        "print(\"Creating relational database...\")\n",
        "\n",
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect('olist_database.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Drop existing tables if they exist (for re-runs)\n",
        "print(\"Clearing existing tables...\")\n",
        "tables = ['reviews', 'payments', 'order_items', 'products', 'sellers', 'customers', 'orders']\n",
        "for table in tables:\n",
        "    cursor.execute(f'DROP TABLE IF EXISTS {table}')\n",
        "conn.commit()\n",
        "\n",
        "# Create tables based on the data structure\n",
        "# Orders table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS orders (\n",
        "    order_id INTEGER PRIMARY KEY,\n",
        "    order_status TEXT,\n",
        "    order_purchase_timestamp TEXT,\n",
        "    order_approved_at TEXT,\n",
        "    order_delivered_customer_date TEXT,\n",
        "    order_delivered_carrier_date TEXT,\n",
        "    order_estimated_delivery_date TEXT,\n",
        "    delivery_time_days REAL\n",
        ")\n",
        "''')\n",
        "\n",
        "# Customers table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS customers (\n",
        "    customer_id TEXT PRIMARY KEY,\n",
        "    customer_unique_id TEXT,\n",
        "    customer_city TEXT,\n",
        "    customer_state TEXT,\n",
        "    customer_zip_code_prefix TEXT,\n",
        "    customer_lat REAL,\n",
        "    customer_lng REAL\n",
        ")\n",
        "''')\n",
        "\n",
        "# Sellers table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS sellers (\n",
        "    seller_id TEXT PRIMARY KEY,\n",
        "    seller_city TEXT,\n",
        "    seller_state TEXT,\n",
        "    seller_zip_code_prefix TEXT,\n",
        "    seller_lat REAL,\n",
        "    seller_lng REAL\n",
        ")\n",
        "''')\n",
        "\n",
        "# Products table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS products (\n",
        "    product_id TEXT PRIMARY KEY,\n",
        "    product_category_name TEXT,\n",
        "    product_category_name_english TEXT,\n",
        "    product_name_lenght REAL,\n",
        "    product_description_lenght REAL,\n",
        "    product_photos_qty REAL,\n",
        "    product_weight_g REAL,\n",
        "    product_length_cm REAL,\n",
        "    product_height_cm REAL,\n",
        "    product_width_cm REAL\n",
        ")\n",
        "''')\n",
        "\n",
        "# Order items table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS order_items (\n",
        "    order_id INTEGER,\n",
        "    order_item_id INTEGER,\n",
        "    product_id TEXT,\n",
        "    seller_id TEXT,\n",
        "    customer_id TEXT,\n",
        "    price REAL,\n",
        "    freight_value REAL,\n",
        "    item_total_value REAL,\n",
        "    shipping_limit_date TEXT,\n",
        "    PRIMARY KEY (order_id, order_item_id),\n",
        "    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n",
        "    FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
        "    FOREIGN KEY (seller_id) REFERENCES sellers(seller_id),\n",
        "    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
        ")\n",
        "''')\n",
        "\n",
        "# Payments table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS payments (\n",
        "    order_id INTEGER,\n",
        "    payment_types TEXT,\n",
        "    total_payment_value REAL,\n",
        "    payment_count REAL,\n",
        "    order_total_value REAL,\n",
        "    PRIMARY KEY (order_id),\n",
        "    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n",
        ")\n",
        "''')\n",
        "\n",
        "# Reviews table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS reviews (\n",
        "    order_id INTEGER,\n",
        "    avg_review_score REAL,\n",
        "    review_count REAL,\n",
        "    review_comments TEXT,\n",
        "    review_emotion TEXT,\n",
        "    review_emotion_intensity REAL,\n",
        "    PRIMARY KEY (order_id),\n",
        "    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n",
        ")\n",
        "''')\n",
        "\n",
        "conn.commit()\n",
        "print(\"Database schema created successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Populate the database tables\n",
        "print(\"Populating database tables...\")\n",
        "\n",
        "# Extract unique records for each table\n",
        "print(\"Extracting unique orders...\")\n",
        "orders_df = df[['order_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', \n",
        "                'order_delivered_customer_date', 'order_delivered_carrier_date', \n",
        "                'order_estimated_delivery_date', 'delivery_time_days']].drop_duplicates(subset=['order_id'])\n",
        "orders_df.to_sql('orders', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Extracting unique customers...\")\n",
        "customers_df = df[['customer_id', 'customer_unique_id', 'customer_city', 'customer_state', \n",
        "                   'customer_zip_code_prefix', 'customer_lat', 'customer_lng']].drop_duplicates(subset=['customer_id'])\n",
        "customers_df.to_sql('customers', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Extracting unique sellers...\")\n",
        "sellers_df = df[['seller_id', 'seller_city', 'seller_state', 'seller_zip_code_prefix', \n",
        "                'seller_lat', 'seller_lng']].drop_duplicates(subset=['seller_id'])\n",
        "sellers_df.to_sql('sellers', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Extracting unique products...\")\n",
        "products_df = df[['product_id', 'product_category_name', 'product_category_name_english', \n",
        "                  'product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
        "                  'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']].drop_duplicates(subset=['product_id'])\n",
        "products_df.to_sql('products', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Extracting order items...\")\n",
        "order_items_df = df[['order_id', 'order_item_id', 'product_id', 'seller_id', 'customer_id',\n",
        "                     'price', 'freight_value', 'item_total_value', 'shipping_limit_date']]\n",
        "order_items_df.to_sql('order_items', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Extracting payments...\")\n",
        "payments_df = df[['order_id', 'payment_types', 'total_payment_value', 'payment_count', \n",
        "                  'order_total_value']].drop_duplicates(subset=['order_id'])\n",
        "payments_df.to_sql('payments', conn, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Extracting reviews...\")\n",
        "reviews_df = df[['order_id', 'avg_review_score', 'review_count', 'review_comments', \n",
        "                 'review_emotion', 'review_emotion_intensity']].drop_duplicates(subset=['order_id'])\n",
        "reviews_df.to_sql('reviews', conn, if_exists='replace', index=False)\n",
        "\n",
        "conn.commit()\n",
        "print(\"Database populated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify database\n",
        "cursor.execute(\"SELECT COUNT(*) FROM orders\")\n",
        "print(f\"Orders in database: {cursor.fetchone()[0]}\")\n",
        "\n",
        "cursor.execute(\"SELECT COUNT(*) FROM customers\")\n",
        "print(f\"Customers in database: {cursor.fetchone()[0]}\")\n",
        "\n",
        "cursor.execute(\"SELECT COUNT(*) FROM sellers\")\n",
        "print(f\"Sellers in database: {cursor.fetchone()[0]}\")\n",
        "\n",
        "cursor.execute(\"SELECT COUNT(*) FROM products\")\n",
        "print(f\"Products in database: {cursor.fetchone()[0]}\")\n",
        "\n",
        "cursor.execute(\"SELECT COUNT(*) FROM order_items\")\n",
        "print(f\"Order items in database: {cursor.fetchone()[0]}\")\n",
        "\n",
        "conn.close()\n",
        "print(\"Database connection closed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Embeddings Generation\n",
        "\n",
        "Now we'll create vector embeddings for all columns to enable RAG functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Load the processed data\n",
        "print(\"Loading processed data...\")\n",
        "df = pd.read_csv('olist_processed.csv', low_memory=False)\n",
        "\n",
        "# Initialize the embedding model\n",
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and effective model\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "# Initialize emotion extraction using a simple Portuguese sentiment lexicon\n",
        "print(\"Setting up emotion extraction for Portuguese reviews...\")\n",
        "\n",
        "# Portuguese emotion/sentiment keywords\n",
        "positive_words = [\n",
        "    'perfeito', 'excelente', 'Ã³timo', 'bom', 'satisfeito', 'gostei', 'adorei', 'recomendo',\n",
        "    'qualidade', 'rÃ¡pido', 'antes do prazo', 'surpreendeu', 'muito bom', 'top', 'show',\n",
        "    'maravilhoso', 'incrÃ­vel', 'fantÃ¡stico', 'amor', 'feliz', 'content', 'satisfeito'\n",
        "]\n",
        "\n",
        "negative_words = [\n",
        "    'ruim', 'pÃ©ssimo', 'horrÃ­vel', 'decepcionado', 'nÃ£o gostei', 'problema', 'defeito',\n",
        "    'quebrado', 'danificado', 'atrasado', 'demorado', 'triste', 'chateado', 'insatisfeito',\n",
        "    'nÃ£o recebi', 'nÃ£o veio', 'faltou', 'errado', 'diferente', 'avariaÃ§Ã£o', 'reclamaÃ§Ã£o'\n",
        "]\n",
        "\n",
        "neutral_words = [\n",
        "    'ok', 'normal', 'regular', 'aceitÃ¡vel', 'bÃ¡sico', 'esperado'\n",
        "]\n",
        "\n",
        "def extract_emotion_from_review(review_text):\n",
        "    \"\"\"Extract emotion from Portuguese review text\"\"\"\n",
        "    if pd.isna(review_text) or not str(review_text).strip():\n",
        "        return 'neutral', 0.0\n",
        "    \n",
        "    review_lower = str(review_text).lower()\n",
        "    \n",
        "    # Count positive and negative words\n",
        "    positive_count = sum(1 for word in positive_words if word in review_lower)\n",
        "    negative_count = sum(1 for word in negative_words if word in review_lower)\n",
        "    \n",
        "    # Check for emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    emojis = emoji_pattern.findall(review_text)\n",
        "    positive_emojis = ['ðŸ˜Š', 'ðŸ˜', 'ðŸ˜', 'ðŸ˜„', 'ðŸ˜ƒ', 'ðŸ‘', 'â¤ï¸', 'ðŸ’¯', 'â­', 'ðŸŒŸ', 'âœ¨']\n",
        "    negative_emojis = ['ðŸ˜¢', 'ðŸ˜ž', 'ðŸ˜ ', 'ðŸ˜¡', 'ðŸ‘Ž', 'ðŸ˜¥', 'ðŸ˜”']\n",
        "    \n",
        "    emoji_score = 0\n",
        "    for emoji in emojis:\n",
        "        if any(pe in emoji for pe in positive_emojis):\n",
        "            emoji_score += 1\n",
        "        elif any(ne in emoji for ne in negative_emojis):\n",
        "            emoji_score -= 1\n",
        "    \n",
        "    # Calculate sentiment score\n",
        "    sentiment_score = positive_count - negative_count + (emoji_score * 0.5)\n",
        "    \n",
        "    # Determine emotion\n",
        "    if sentiment_score > 1:\n",
        "        emotion = 'positive'\n",
        "        intensity = min(1.0, abs(sentiment_score) / 5.0)\n",
        "    elif sentiment_score < -1:\n",
        "        emotion = 'negative'\n",
        "        intensity = min(1.0, abs(sentiment_score) / 5.0)\n",
        "    else:\n",
        "        emotion = 'neutral'\n",
        "        intensity = 0.5\n",
        "    \n",
        "    return emotion, intensity\n",
        "\n",
        "# Extract emotions from reviews\n",
        "print(\"Extracting emotions from reviews...\")\n",
        "emotions = []\n",
        "intensities = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting emotions\"):\n",
        "    emotion, intensity = extract_emotion_from_review(row.get('review_comments', ''))\n",
        "    emotions.append(emotion)\n",
        "    intensities.append(intensity)\n",
        "\n",
        "df['review_emotion'] = emotions\n",
        "df['review_emotion_intensity'] = intensities\n",
        "\n",
        "print(f\"Emotion distribution:\")\n",
        "print(df['review_emotion'].value_counts())\n",
        "print(\"\\nEmotion extraction complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive text representations for each row using ALL columns\n",
        "print(\"Creating text representations from all columns...\")\n",
        "\n",
        "def create_comprehensive_text(row):\n",
        "    \"\"\"Create a comprehensive text representation using all columns\"\"\"\n",
        "    text_parts = []\n",
        "    \n",
        "    # Order information\n",
        "    text_parts.append(f\"Order ID: {row['order_id']}\")\n",
        "    text_parts.append(f\"Order Status: {row['order_status']}\")\n",
        "    text_parts.append(f\"Purchase Date: {row['order_purchase_timestamp']}\")\n",
        "    text_parts.append(f\"Approved Date: {row['order_approved_at']}\")\n",
        "    text_parts.append(f\"Delivery Date: {row['order_delivered_customer_date']}\")\n",
        "    text_parts.append(f\"Estimated Delivery: {row['order_estimated_delivery_date']}\")\n",
        "    text_parts.append(f\"Delivery Time: {row['delivery_time_days']} days\")\n",
        "    \n",
        "    # Customer information\n",
        "    text_parts.append(f\"Customer ID: {row['customer_id']}\")\n",
        "    text_parts.append(f\"Customer Unique ID: {row['customer_unique_id']}\")\n",
        "    text_parts.append(f\"Customer City: {row['customer_city']}\")\n",
        "    text_parts.append(f\"Customer State: {row['customer_state']}\")\n",
        "    text_parts.append(f\"Customer Zip: {row['customer_zip_code_prefix']}\")\n",
        "    text_parts.append(f\"Customer Location: ({row['customer_lat']}, {row['customer_lng']})\")\n",
        "    \n",
        "    # Seller information\n",
        "    text_parts.append(f\"Seller ID: {row['seller_id']}\")\n",
        "    text_parts.append(f\"Seller City: {row['seller_city']}\")\n",
        "    text_parts.append(f\"Seller State: {row['seller_state']}\")\n",
        "    text_parts.append(f\"Seller Zip: {row['seller_zip_code_prefix']}\")\n",
        "    text_parts.append(f\"Seller Location: ({row['seller_lat']}, {row['seller_lng']})\")\n",
        "    \n",
        "    # Product information\n",
        "    text_parts.append(f\"Product ID: {row['product_id']}\")\n",
        "    text_parts.append(f\"Product Category: {row['product_category_name']} ({row['product_category_name_english']})\")\n",
        "    text_parts.append(f\"Product Name Length: {row['product_name_lenght']}\")\n",
        "    text_parts.append(f\"Product Description Length: {row['product_description_lenght']}\")\n",
        "    text_parts.append(f\"Product Photos: {row['product_photos_qty']}\")\n",
        "    text_parts.append(f\"Product Weight: {row['product_weight_g']}g\")\n",
        "    text_parts.append(f\"Product Dimensions: {row['product_length_cm']}cm x {row['product_width_cm']}cm x {row['product_height_cm']}cm\")\n",
        "    \n",
        "    # Order item information\n",
        "    text_parts.append(f\"Order Item ID: {row['order_item_id']}\")\n",
        "    text_parts.append(f\"Price: R$ {row['price']}\")\n",
        "    text_parts.append(f\"Freight Value: R$ {row['freight_value']}\")\n",
        "    text_parts.append(f\"Item Total: R$ {row['item_total_value']}\")\n",
        "    text_parts.append(f\"Shipping Limit: {row['shipping_limit_date']}\")\n",
        "    \n",
        "    # Payment information\n",
        "    text_parts.append(f\"Payment Type: {row['payment_types']}\")\n",
        "    text_parts.append(f\"Total Payment: R$ {row['total_payment_value']}\")\n",
        "    text_parts.append(f\"Order Total: R$ {row['order_total_value']}\")\n",
        "    text_parts.append(f\"Payment Count: {row['payment_count']}\")\n",
        "    \n",
        "    # Review information\n",
        "    text_parts.append(f\"Review Score: {row['avg_review_score']}\")\n",
        "    text_parts.append(f\"Review Count: {row['review_count']}\")\n",
        "    if pd.notna(row['review_comments']) and str(row['review_comments']).strip():\n",
        "        text_parts.append(f\"Review Comment: {row['review_comments']}\")\n",
        "    \n",
        "    # Emotion information\n",
        "    if 'review_emotion' in row:\n",
        "        text_parts.append(f\"Review Emotion: {row['review_emotion']}\")\n",
        "        text_parts.append(f\"Emotion Intensity: {row['review_emotion_intensity']:.2f}\")\n",
        "    \n",
        "    return \" | \".join([str(part) for part in text_parts if pd.notna(part)])\n",
        "\n",
        "# Create text representations\n",
        "print(\"Generating text representations...\")\n",
        "texts = []\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
        "    text = create_comprehensive_text(row)\n",
        "    texts.append(text)\n",
        "\n",
        "print(f\"Created {len(texts)} text representations\")\n",
        "print(f\"Sample text length: {len(texts[0])} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings in batches\n",
        "print(\"Generating vector embeddings...\")\n",
        "batch_size = 100\n",
        "embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "    embeddings.append(batch_embeddings)\n",
        "\n",
        "embeddings = np.vstack(embeddings)\n",
        "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# Save embeddings and metadata\n",
        "print(\"Saving embeddings...\")\n",
        "np.save('embeddings.npy', embeddings)\n",
        "\n",
        "# Save text representations and corresponding row indices\n",
        "metadata = {\n",
        "    'texts': texts,\n",
        "    'row_indices': list(range(len(texts)))\n",
        "}\n",
        "\n",
        "with open('embeddings_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"Embeddings saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a vector store index using FAISS for efficient similarity search\n",
        "try:\n",
        "    import faiss\n",
        "    \n",
        "    print(\"Creating FAISS index...\")\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
        "    \n",
        "    # Normalize embeddings for cosine similarity (optional but often better)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    \n",
        "    # Save the index\n",
        "    faiss.write_index(index, 'embeddings_index.faiss')\n",
        "    print(\"FAISS index created and saved!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"FAISS not available, using numpy-based search instead\")\n",
        "    # We'll use numpy-based search in the RAG app if FAISS is not available\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
